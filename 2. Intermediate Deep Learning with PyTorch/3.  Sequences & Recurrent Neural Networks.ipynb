{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Sequences & Recurrent Neural Networks\n",
    "\n",
    "Build and train recurrent neural networks (RNNs) for processing sequential data such as time series, text, or audio. You will learn about the two most popular recurrent architectures, Long-Short Term Memory (LSTM) and Gated Recurrent Unit (GRU) networks, as well as how to prepare sequential data for model training. You will practice your skills by training and evaluating a recurrent model for predicting electricity consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchmetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating sequences\n",
    "\n",
    "To be able to train neural networks on sequential data, you need to pre-process it first. You'll chunk the data into inputs-target pairs, where the inputs are some number of consecutive data points and the target is the next data point.\n",
    "\n",
    "Your task is to define a function to do this called `create_sequences()`. As inputs, it will receive data stored in a DataFrame, `df` and `seq_length`, the length of the inputs. As outputs, it should return two NumPy arrays, one with input sequences and the other one with the corresponding targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>consumption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-01-01 00:15:00</td>\n",
       "      <td>-0.704319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-01-01 00:30:00</td>\n",
       "      <td>-0.704319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-01-01 00:45:00</td>\n",
       "      <td>-0.678983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-01-01 01:00:00</td>\n",
       "      <td>-0.653647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-01-01 01:15:00</td>\n",
       "      <td>-0.704319</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             timestamp  consumption\n",
       "0  2011-01-01 00:15:00    -0.704319\n",
       "1  2011-01-01 00:30:00    -0.704319\n",
       "2  2011-01-01 00:45:00    -0.678983\n",
       "3  2011-01-01 01:00:00    -0.653647\n",
       "4  2011-01-01 01:15:00    -0.704319"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"datasets/electricity_consump/electricity_train.csv\")\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions:\n",
    "\n",
    "- Iterate over the range of the number of data points minus the length of an input sequence.\n",
    "- Define the inputs `x` as the slice of `df` from the ith row to the `i + seq_lengthth` row and the column at index `1`.\n",
    "- Define the target `y` as the slice of `df` at row index `i + seq_length` and the column at index `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(df: pd.DataFrame, seq_length: int):\n",
    "    xs, ys = [], []\n",
    "    # Iterate over data indices\n",
    "    for i in range(len(df) - seq_length):\n",
    "        # Define inputs\n",
    "        x = df.iloc[i : (i + seq_length), 1]\n",
    "        # Define target\n",
    "        y = df.iloc[i + seq_length, 1]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential Dataset\n",
    "\n",
    "Just like tabular and image data, sequential data is easiest passed to a model through a torch Dataset and DataLoader. To build a sequential Dataset, you will call `create_sequences()` to get the NumPy arrays with inputs and targets, and inspect their shape. Next, you will pass them to a `TensorDataset` to create a proper torch Dataset, and inspect its length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions:\n",
    "\n",
    "- Call `create_sequences()`, passing it the training DataFrame and a sequence length of 24*4, assigning the result to X_train, y_train.\n",
    "- Define `dataset_train` by calling TensorDataset and passing it two arguments, the inputs and the targets created by `create_sequences()`, both converted from NumPy arrays to tensors of floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(105119, 96) (105119,)\n",
      "105119\n"
     ]
    }
   ],
   "source": [
    "# Use create_sequences to create inputs and targets\n",
    "X_train, y_train = create_sequences(train_data, 24 * 4)\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "# Create TensorDataset\n",
    "dataset_train = TensorDataset(\n",
    "    torch.from_numpy(X_train).float(),\n",
    "    torch.from_numpy(y_train).float(),\n",
    ")\n",
    "print(len(dataset_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a forecasting RNN\n",
    "\n",
    "It's time to build your first recurrent network! It will be a sequence-to-vector model consisting of an RNN layer with two layers and a `hidden_size` of `32`. After the RNN layer, a simple linear layer will map the outputs to a single value to be predicted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions:\n",
    "\n",
    "- Define the RNN layer passing it the correct values for `input_size`, `hidden_size`, `num_layers`, and `batch_first`, and assign it to `self.rnn`\n",
    "- Initialize the first hidden state `h0` as a tensor of zeros of the appropriate shape.\n",
    "- Pass the input `x` and the first hidden state `h0` through recurrent layer.\n",
    "- Pass recurrent layer's last output through the linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Define RNN layer\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=1,\n",
    "            hidden_size=32,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.fc = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize first hidden state with zeros\n",
    "        h0 = torch.zeros(2, x.size(0), 32)\n",
    "        # Pass x and h0 through recurrent layer\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        # Pass recurrent layer's last output through linear layer\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM and GRU cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM network\n",
    "\n",
    "As you already know, plain RNN cells are not used that much in practice. A more frequently used alternative that ensures a much better handling of long sequences are Long Short-Term Memory cells, or LSTMs. In this exercise, you will be build an LSTM network yourself!\n",
    "\n",
    "The most important implementation difference from the RNN network you have built previously comes from the fact that LSTMs have two rather than one hidden states. This means you will need to initialize this additional hidden state and pass it to the LSTM cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions:\n",
    "\n",
    "- In the `.__init__()` method, define an LSTM layer and assign it to `self.lstm`.\n",
    "- In the `forward()` method, initialize the first long-term memory hidden state `c0` with zeros.\n",
    "- In the `forward()` method, pass all three inputs to the `LSTM` layer: the current time step's inputs, and a tuple containing the two hidden states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        # Define lstm layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=1,\n",
    "            hidden_size=32,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.fc = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(2, x.size(0), 32)\n",
    "        # Initialize long-term memory\n",
    "        c0 = torch.zeros(2, x.size(0), 32)\n",
    "        # Pass all inputs to lstm layer\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU network\n",
    "\n",
    "Next to LSTMs, another popular recurrent neural network variant is the Gated Recurrent Unit, or GRU. It's appeal is in its simplicity: GRU cells require less computation than LSTM cells while often matching them in performance.\n",
    "\n",
    "The code you are provided with is the RNN model definition that you coded previously. Your task is to adapt it such that it produces a GRU network instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Define RNN layer\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=1,\n",
    "            hidden_size=32,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.fc = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(2, x.size(0), 32)\n",
    "        out, _ = self.rnn(x, h0)  \n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions:\n",
    "\n",
    "- Update the RNN model definition in order to obtain a GRU network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Define RNN layer\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=1,\n",
    "            hidden_size=32,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.fc = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(2, x.size(0), 32)\n",
    "        out, _ = self.gru(x, h0)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and evaluating RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN training loop\n",
    "\n",
    "It's time to train the electricity consumption forecasting model!\n",
    "\n",
    "You will use the LSTM network you have defined previously which is available to you as `Net`, as is the `dataloader_train` you built before.\n",
    "\n",
    "In this exercise, you will train the model for only three epochs to make sure the training progresses as expected. Let's get to it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Define lstm layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=1,\n",
    "            hidden_size=32,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.fc = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(2, x.size(0), 32)\n",
    "        # Initialize long-term memory\n",
    "        c0 = torch.zeros(2, x.size(0), 32)\n",
    "        # Pass all inputs to lstm layer\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train = DataLoader(dataset_train, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions:\n",
    "\n",
    "- Set up the Mean Squared Error loss and assign it to `criterion`.\n",
    "- Reshape `seqs` to `(batch size, sequence length, num features)`, which in our case is `(16, 96, 1)`, and re-assign the result to `seqs`.\n",
    "- Pass `seqs` to the model to get its `outputs`.\n",
    "- Based on previously computed quantities, calculate the loss, assigning it to `loss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "# Set up MSE loss\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0001)\n",
    "\n",
    "for epoch in range(3):\n",
    "    for seqs, labels in dataloader_train:\n",
    "        # Reshape model inputs\n",
    "        seqs = seqs.view(16, 96, 1)\n",
    "        # Get model outputs\n",
    "        outputs = net(seqs)\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating forecasting models\n",
    "\n",
    "It's evaluation time! The same LSTM network that you have trained in the previous exercise has been trained for you for a few more epochs and is available as `net`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>consumption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-01-01 00:00:00</td>\n",
       "      <td>-0.932595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-01-01 00:15:00</td>\n",
       "      <td>-0.957931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-01-01 00:30:00</td>\n",
       "      <td>-0.932595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014-01-01 00:45:00</td>\n",
       "      <td>-0.907259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-01-01 01:00:00</td>\n",
       "      <td>-0.881923</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             timestamp  consumption\n",
       "0  2014-01-01 00:00:00    -0.932595\n",
       "1  2014-01-01 00:15:00    -0.957931\n",
       "2  2014-01-01 00:30:00    -0.932595\n",
       "3  2014-01-01 00:45:00    -0.907259\n",
       "4  2014-01-01 01:00:00    -0.881923"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv(\"datasets/electricity_consump/electricity_test.csv\")\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34944, 96) (34944,)\n",
      "34944\n"
     ]
    }
   ],
   "source": [
    "# Use create_sequences to create inputs and targets\n",
    "X_test, y_test = create_sequences(test_data, 24 * 4)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "# Create TensorDataset\n",
    "dataset_test = TensorDataset(\n",
    "    torch.from_numpy(X_test).float(),\n",
    "    torch.from_numpy(y_test).float(),\n",
    ")\n",
    "print(len(dataset_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_test = DataLoader(dataset=dataset_test, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions:\n",
    "\n",
    "- Define the Mean Squared Error metrics and assign it to `mse`.\n",
    "- Pass the input sequence to `net`, and squeeze the result before you assign it to `outputs`.\n",
    "- Compute the final value of the test metric assigning it to `test_mse`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 0.14219625294208527\n"
     ]
    }
   ],
   "source": [
    "# Define MSE metric\n",
    "mse = torchmetrics.MeanSquaredError()\n",
    "\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    for seqs, labels in dataloader_test:\n",
    "        seqs = seqs.view(32, 96, 1)\n",
    "        # Pass seqs to net and squeeze the result\n",
    "        outputs = net(seqs).squeeze()\n",
    "        mse(outputs, labels)\n",
    "\n",
    "# Compute final metric value\n",
    "test_mse = mse.compute()\n",
    "print(f\"Test MSE: {test_mse}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning-track-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
