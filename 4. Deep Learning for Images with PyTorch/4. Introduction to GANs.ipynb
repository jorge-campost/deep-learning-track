{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Image Generation with GANs\n",
    "\n",
    "Generate completely new images with Generative Adversarial Networks (GANs). Learn to build and train a Deep Convolutional GAN, and how to evaluate the quality and variety of its outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to GANs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator\n",
    "\n",
    "A GAN generator takes a random noise vector as input and produces a generated image. To make its architecture more reusable, you will pass both input and output shapes as parameters to the model. This way, you can use the same model with different sizes of input noise and images of varying shapes.\n",
    "\n",
    "You can also access a custom `gen_block()` function which returns a block of: linear layer, batch norm, and ReLU activation. You will use it as a building block for the generator.\n",
    "\n",
    "```\n",
    "def gen_block(in_dim, out_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(in_dim, out_dim),\n",
    "        nn.BatchNorm1d(out_dim),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_block(in_dim, out_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(in_dim, out_dim), nn.BatchNorm1d(out_dim), nn.ReLU(inplace=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions:\n",
    "\n",
    "- Define `self.generator` as a sequential model.\n",
    "- After the last `gen_block`, add a linear layer with the appropriate input size and the output size of `out_dim`.\n",
    "- Add a sigmoid activation after the linear layer.\n",
    "- In the `forward()` method, pass the model's input through `self.generator`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        # Define generator block\n",
    "        self.generator = nn.Sequential(\n",
    "            gen_block(in_dim, 256),\n",
    "            gen_block(256, 512),\n",
    "            gen_block(512, 1024),\n",
    "            # Add linear layer\n",
    "            nn.Linear(1024, out_dim),\n",
    "            # Add activation\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass input through generator\n",
    "        return self.generator(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator\n",
    "\n",
    "With the generator defined, the next step in building a GAN is to construct the discriminator. It takes the generator's output as input, and produces a binary prediction: is the input generated or real?\n",
    "\n",
    "You can also access a custom `disc_block()` function which returns a block of a linear layer followed by a LeakyReLU activation. You will use it as a building block for the discriminator.\n",
    "\n",
    "```\n",
    "def disc_block(in_dim, out_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(in_dim, out_dim),\n",
    "        nn.LeakyReLU(0.2)\n",
    "    )\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disc_block(in_dim, out_dim):\n",
    "    return nn.Sequential(nn.Linear(in_dim, out_dim), nn.LeakyReLU(0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions:\n",
    "\n",
    "- Add the last discriminator block to the model, with the appropriate input size and the output of `256`.\n",
    "- After the last discriminator block, add a linear layer to map the output to the size of `1`.\n",
    "- Define the forward() method to pass the input image through the sequential block defined in `__init__()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, im_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            disc_block(im_dim, 1024),\n",
    "            disc_block(1024, 512),\n",
    "            # Define last discriminator block\n",
    "            disc_block(512, 256),\n",
    "            # Add a linear layer\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Define the forward method\n",
    "        return self.disc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Convolutional GAN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Generator\n",
    "\n",
    "Define a convolutional generator following the DCGAN guidelines discussed in the last video.\n",
    "\n",
    "A custom function `dc_gen_block()` is available, which eturns a block of a transposed convolution, batch norm, and ReLU activation. This function serves as a foundational component for constructing the convolutional generator. You can get familiar with `dc_gen_block()`'s definition below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dc_gen_block(in_dim, out_dim, kernel_size, stride):\n",
    "    return nn.Sequential(\n",
    "        nn.ConvTranspose2d(in_dim, out_dim, kernel_size, stride=stride),\n",
    "        nn.BatchNorm2d(out_dim),\n",
    "        nn.ReLU(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions:\n",
    "\n",
    "- Add the last generator block, mapping the size of the feature maps to `256`.\n",
    "- Add a transposed convolution with the output size of `3`.\n",
    "- Add the tanh activation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCGenerator(nn.Module):\n",
    "    def __init__(self, in_dim, kernel_size=4, stride=2):\n",
    "        super(DCGenerator, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.gen = nn.Sequential(\n",
    "            dc_gen_block(in_dim, 1024, kernel_size, stride),\n",
    "            dc_gen_block(1024, 512, kernel_size, stride),\n",
    "            # Add last generator block\n",
    "            dc_gen_block(512, 256, kernel_size, stride),\n",
    "            # Add transposed convolution\n",
    "            nn.ConvTranspose2d(256, 3, kernel_size, stride),\n",
    "            # Add tanh activation\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(len(x), self.in_dim, 1, 1)\n",
    "        return self.gen(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Discriminator\n",
    "\n",
    "With the DCGAN's generator ready, the last step before you can proceed to training it is to define the convolutional discriminator.\n",
    "\n",
    "To build the convolutional discriminator, you will use a custom `gc_disc_block()` function which returns a block of a convolution followed by a batch norm and the leaky ReLU activation. You can inspect `dc_disc_block()`'s definition below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dc_disc_block(in_dim, out_dim, kernel_size, stride):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_dim, out_dim, kernel_size, stride=stride),\n",
    "        nn.BatchNorm2d(out_dim),\n",
    "        nn.LeakyReLU(0.2),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions:\n",
    "\n",
    "- Add the first discriminator block using the custom `dc_disc_block()` function with 3 input feature maps and 512 output feature maps.\n",
    "- Add the convolutional layer with the output size of `1`.\n",
    "- In the `forward()` method, pass the input through the sequential block you defined in `__init__()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCDiscriminator(nn.Module):\n",
    "    def __init__(self, kernel_size=4, stride=2):\n",
    "        super(DCDiscriminator, self).__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            # Add first discriminator block\n",
    "            dc_disc_block(3, 512, kernel_size, stride),\n",
    "            dc_disc_block(512, 1024, kernel_size, stride),\n",
    "            # Add a convolution\n",
    "            nn.Conv2d(1024, 1, kernel_size, stride=stride),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass input through sequential block\n",
    "        x = self.disc(x)\n",
    "        return x.view(len(x), -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training GANs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator loss\n",
    "\n",
    "Before you can train your GAN, you need to define loss functions for both the generator and the discriminator. You will start with the former.\n",
    "\n",
    "Recall that the generator's job is to produce such fake images that would fool the discriminator into classifying them as real. Therefore, the generator incurs a loss if the images it generated are classified by the discriminator as fake (label `0`).\n",
    "\n",
    "Define the `gen_loss()` function that calculates the generator loss. It takes four arguments:\n",
    "\n",
    "- `gen`, the generator model\n",
    "- `disc`, the discriminator model\n",
    "- `num_images`, the number of images in batch\n",
    "- `z_dim`, the size of the input random noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions:\n",
    "\n",
    "- Generate random noise of shape `num_images` by `z_dim` and assign it to noise.\n",
    "- Use the generator to generate a fake image from for `noise` and assign it to `fake`.\n",
    "- Get discriminator's prediction for the generated fake image.\n",
    "- Compute generators loss by calling `criterion` on discriminator's predictions and the a tensor of ones of the same shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_loss(gen, disc, criterion, num_images, z_dim):\n",
    "    # Define random noise\n",
    "    noise = torch.rand(num_images, z_dim)\n",
    "    # Generate fake image\n",
    "    fake = gen(noise)\n",
    "    # Get discriminator's prediction on the fake image\n",
    "    disc_pred = disc(fake)\n",
    "    # Compute generator loss\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    gen_loss = criterion(fake, disc_pred)\n",
    "    return gen_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator loss\n",
    "\n",
    "It's time to define the loss for the discriminator. Recall that the discriminator's job is to classify images either real or fake. Therefore, the generator incurs a loss if it classifies generator's outputs as real (label `1`) or the real images as fake (label `0`).\n",
    "\n",
    "Define the `disc_loss()` function that calculates the discriminator loss. It takes five arguments:\n",
    "\n",
    "- `gen`, the generator model\n",
    "- `disc`, the discriminator model\n",
    "- `real`, a sample of real images from the training data\n",
    "- `num_images`, the number of images in batch\n",
    "- `z_dim`, the size of the input random noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions:\n",
    "\n",
    "- Use the discriminator to classify `fake` images and assign the predictions to `disc_pred_fake`.\n",
    "- Compute the fake loss component by calling `criterion` on discriminator's predictions for fake images and the a tensor of zeros of the same shape.\n",
    "- Use the discriminator to classify `real` images and assign the predictions to `disc_pred_real`.\n",
    "- Compute the real loss component by calling `criterion` on discriminator's predictions for real images and the a tensor of ones of the same shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disc_loss(gen, disc, real, num_images, z_dim):\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    noise = torch.randn(num_images, z_dim)\n",
    "    fake = gen(noise)\n",
    "    # Get discriminator's predictions for fake images\n",
    "    disc_pred_fake = disc(fake)\n",
    "    # Calculate the fake loss component\n",
    "    fake_loss = criterion(fake, disc_pred_fake)\n",
    "    # Get discriminator's predictions for real images\n",
    "    disc_pred_real = disc(real)\n",
    "    # Calculate the real loss component\n",
    "    real_loss = criterion(real, disc_pred_real)\n",
    "    disc_loss = (real_loss + fake_loss) / 2\n",
    "    return disc_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "\n",
    "Finally, all the hard work you put into defining the model architectures and loss functions comes to fruition: it's training time! Your job is to implement and execute the GAN training loop. Note: a `break` statement is placed after the first batch of data to avoid a long runtime.\n",
    "\n",
    "The two optimizers, `disc_opt` and `gen_opt`, have been initialized as `Adam()` optimizers. The functions to compute the losses that you defined earlier, `gen_loss()` and `disc_loss()`, are available to you. A `dataloader` is also prepared for you.\n",
    "\n",
    "Recall that:\n",
    "\n",
    "- `disc_loss()`'s arguments are: `gen`, `disc`, `real`, `cur_batch_size`, `z_dim`.\n",
    "- `gen_loss()`'s arguments are: `gen`, `disc`, `cur_batch_size`, `z_dim`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions:\n",
    "- Calculate the discriminator loss using `disc_loss()` by passing it the generator, the discriminator, the sample of real images, current batch size, and the noise size of `16`, in this order, and assign the result to `disc_loss`.\n",
    "- Calculate gradients using `disc_loss`.\n",
    "- Calculate the generator loss using `gen_loss()` by passing it the generator, the discriminator, current batch size, and the noise size of `16`, in this order, and assign the result to `gen_loss`.\n",
    "- Calculate gradients using `gen_loss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1):\n",
    "    for real in dataloader:\n",
    "        cur_batch_size = len(real)\n",
    "        \n",
    "        disc_opt.zero_grad()\n",
    "        # Calculate discriminator loss\n",
    "        disc_loss = disc_loss(gen, disc, real, cur_batch_size, z_dim=16)\n",
    "        # Compute discriminator gradients \n",
    "        disc_loss.backward()\n",
    "        disc_opt.step()\n",
    "\n",
    "        gen_opt.zero_grad()\n",
    "        # Calculate generator loss\n",
    "        gen_loss = gen_loss(gen, disc, cur_batch_size, z_dim=16)\n",
    "        # Compute generator gradients\n",
    "        gen_loss.backward()\n",
    "        gen_opt.step()\n",
    "\n",
    "        print(f\"Generator loss: {gen_loss}\")\n",
    "        print(f\"Discriminator loss: {disc_loss}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating GANs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating images\n",
    "\n",
    "Now that you have designed and trained your GAN, it's time to evaluate the quality of the images it can generate. For a start, you will perform a visual inspection to see if the generation resemble the Pokemons at all. To do this, you will create random noise as input for the generator, pass it to the model and plot the outputs.\n",
    "\n",
    "The Deep Convolutional Generator with trained weights is available to you as `gen`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions:\n",
    "\n",
    "- Create a random noise tensor of shape `num_images_to_generate` by `16`, the input noise size you used to train the generator, and assign it to `noise`.\n",
    "- Generate images by passing the noise to the generator and assign them to `fake`.\n",
    "- Inside the for loop, slice `fake` to extract the i-th image and assign it to `image_tensor`.\n",
    "- Permute `image_tensor`'s dimensions from (color, height, width) to (hight, width, color) and assign the output to `image_tensor_permuted`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images_to_generate = 5\n",
    "# Create random noise tensor\n",
    "noise = torch.rand(5, 16)\n",
    "\n",
    "# Generate images\n",
    "with torch.no_grad():\n",
    "    fake = gen(noise)\n",
    "print(f\"Generated tensor shape: {fake.shape}\")\n",
    "\n",
    "for i in range(num_images_to_generate):\n",
    "    # Slice fake to select i-th image\n",
    "    image_tensor = fake[i, :, :, :]\n",
    "    # Permute the image dimensions\n",
    "    image_tensor_permuted = image_tensor.permute(1, 2, 0)\n",
    "    plt.imshow(image_tensor_permuted)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fréchet Inception Distance\n",
    "\n",
    "The visual inspection of generated images is a great start. But given they look okay, a more precise, quantitative evaluation will be helpful to understand the generator's performance. You will evaluate your GAN using the Fréchet Inception Distance, or FID.\n",
    "\n",
    "Two tensors with fake and real images, 32 examples each, are available to you as `fake` and `real`, respectively. Use them to compute the FID!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions:\n",
    "\n",
    "- Import `FrechetInceptionDistance` from the appropriate `torchmetrics` module.\n",
    "- Instantiate the FID metric based on the 64th Inception feature layer and assign it to `fid`.\n",
    "- Update `fid` with real image tensor, multiplied by `255` and parsed to `torch.uint8`.\n",
    "- Compute the `fid` metric, assigning the output to `fid_score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import FrechetInceptionDistance\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "\n",
    "# Instantiate FID\n",
    "fid = FrechetInceptionDistance(feature=64)\n",
    "\n",
    "# Update FID with real images\n",
    "fid.update((fake * 255).to(torch.uint8), real=False)\n",
    "fid.update((real * 255).to(torch.uint8), real=True)\n",
    "\n",
    "# Compute the metric\n",
    "fid_score = fid.compute()\n",
    "print(fid_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning-track-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
